# -*- coding: utf-8 -*-
"""
Task 3: æ€§èƒ½åŸºå‡†æµ‹è¯• (Performance Benchmark)

åŠŸèƒ½æè¿°:
    1. è‡ªåŠ¨ä¸‹è½½å¹¶å¯¹æ¯” YOLOv8n, YOLOv8s, YOLOv8m ä¸‰ä¸ªä¸åŒå°ºå¯¸çš„æ¨¡å‹
    2. åœ¨åŒä¸€æµ‹è¯•é›†ä¸Šå¾ªç¯æµ‹è¯•
    3. è®°å½•å¹¶è®¡ç®—ï¼šFPSï¼ˆæ¨ç†é€Ÿåº¦ï¼‰ã€mAP50-95ï¼ˆå‡†ç¡®ç‡ï¼‰ã€æ¨¡å‹å‚æ•°é‡ (Params)ã€æ¨¡å‹å¤§å° (Size)
    4. ç”Ÿæˆ Markdown æ ¼å¼çš„æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
    5. æ™ºèƒ½åˆ†æå¹¶æ¨èæœ€ä½³æ¨¡å‹

ä½¿ç”¨æ–¹æ³•:
    python task3.py --data data/custom_dataset/dataset.yaml

ä½œè€…: my_yolo Team
æ—¥æœŸ: 2023-12-22
"""

import os
import sys
import time
import argparse
import logging
import torch
import pandas as pd
from pathlib import Path
from typing import List, Dict

try:
    from ultralytics import YOLO
except ImportError:
    print("âŒ Error: 'ultralytics' not found. Please install requirements.")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ModelBenchmark:
    """YOLOv8 æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""

    def __init__(self, data_yaml: str, results_dir: str = 'results/task3'):
        self.data_yaml = data_yaml
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # å®šä¹‰è¦å¯¹æ¯”çš„æ¨¡å‹åˆ—è¡¨
        self.models_to_test = ['yolov8n.pt', 'yolov8s.pt', 'yolov8m.pt']
        
        # ç»“æœå­˜å‚¨
        self.benchmark_results = []

    def run_benchmark(self):
        """æ‰§è¡ŒåŸºå‡†æµ‹è¯•ä¸»å¾ªç¯"""
        if not os.path.exists(self.data_yaml):
            logger.warning(f"âš ï¸ Dataset config not found: {self.data_yaml}")
            logger.warning("âš ï¸ Switching to 'coco128.yaml' for demonstration purposes.")
            self.data_yaml = 'coco128.yaml'  # é™çº§æ–¹æ¡ˆ

        logger.info(f"ğŸš€ Starting benchmark on dataset: {self.data_yaml}")
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"ğŸ’» Compute Device: {device.upper()}")

        for model_name in self.models_to_test:
            self._test_single_model(model_name, device)
            
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report()

    def _test_single_model(self, model_name: str, device: str):
        """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        logger.info(f"\nğŸ§ª Testing model: {model_name}...")
        
        try:
            # 1. åŠ è½½æ¨¡å‹
            model = YOLO(model_name)
            
            # 2. è·å–æ¨¡å‹åŸºç¡€ä¿¡æ¯
            # model.info() è¿”å› (layers, params, gradients, flops)
            # ä½†æˆ‘ä»¬éœ€è¦æ›´ç›´è§‚çš„å±æ€§ï¼Œéƒ¨åˆ†å¯ä»¥é€šè¿‡ model.model.parameters() è®¡ç®—
            params_cnt = sum(p.numel() for p in model.model.parameters()) / 1e6  # Million
            
            # è®¡ç®—æ¨¡å‹æ–‡ä»¶å¤§å° (MB)
            # å¦‚æœæ˜¯è‡ªåŠ¨ä¸‹è½½çš„ï¼Œæƒé‡é€šå¸¸åœ¨å½“å‰ç›®å½•
            if os.path.exists(model_name):
                model_size = os.path.getsize(model_name) / 1e6 # MB
            else:
                model_size = 0.0 # æ— æ³•è·å–

            # 3. è¯„ä¼°å‡†ç¡®ç‡ (mAP)
            logger.info("   Running validation to measure mAP...")
            val_results = model.val(data=self.data_yaml, split='val', verbose=False, device=device)
            map50_95 = val_results.box.map    # mAP50-95
            map50 = val_results.box.map50     # mAP50

            # 4. è¯„ä¼°æ¨ç†é€Ÿåº¦ (FPS)
            # ä½¿ç”¨ val æ¨¡å¼çš„ speed å±æ€§ï¼Œæˆ–è€…æ‰‹åŠ¨è·‘ predict
            # val_results.speed åŒ…å« {'preprocess': t1, 'inference': t2, 'loss': t3, 'postprocess': t4} (ms)
            inference_time_ms = val_results.speed['inference']
            fps = 1000.0 / (inference_time_ms + val_results.speed['preprocess'] + val_results.speed['postprocess'])

            logger.info(f"   âœ… {model_name} Results: mAP={map50_95:.3f}, FPS={fps:.1f}")

            # è®°å½•ç»“æœ
            self.benchmark_results.append({
                'Model': model_name,
                'Size (MB)': round(model_size, 2),
                'Params (M)': round(params_cnt, 2),
                'mAP 50-95': round(map50_95, 3),
                'mAP 50': round(map50, 3),
                'Inference (ms)': round(inference_time_ms, 2),
                'FPS': round(fps, 1)
            })
            
            # æ¸…ç†æ˜¾å­˜
            del model
            if device == 'cuda':
                torch.cuda.empty_cache()

        except Exception as e:
            logger.error(f"âŒ Failed to test {model_name}: {e}")

    def _generate_report(self):
        """ç”Ÿæˆ Markdown æŠ¥å‘Šå’Œåˆ†æå»ºè®®"""
        if not self.benchmark_results:
            logger.error("âŒ No results to report.")
            return

        df = pd.DataFrame(self.benchmark_results)
        
        # 1. ç”Ÿæˆ Markdown è¡¨æ ¼
        md_table = df.to_markdown(index=False)
        
        # 2. æ™ºèƒ½åˆ†æ
        best_acc_model = df.loc[df['mAP 50-95'].idxmax()]
        fastest_model = df.loc[df['FPS'].idxmax()]
        
        # ç®€å•æ¨èé€»è¾‘ï¼šé¦–å…ˆæ»¡è¶³å®æ—¶æ€§(FPS>30)ï¼Œç„¶åé€‰mAPæœ€é«˜çš„
        realtime_models = df[df['FPS'] >= 30]
        if not realtime_models.empty:
            recommended_model = realtime_models.loc[realtime_models['mAP 50-95'].idxmax()]
            reason = "å®ƒåœ¨ä¿æŒå®æ—¶æ€§èƒ½ (FPS > 30) çš„åŒæ—¶æä¾›äº†æœ€é«˜çš„å‡†ç¡®ç‡ã€‚"
        else:
            recommended_model = fastest_model
            reason = "å½“å‰æ²¡æœ‰æ¨¡å‹æ»¡è¶³ 30 FPSï¼Œæ¨èæœ€å¿«çš„æ¨¡å‹ä»¥ä¿è¯æµç•…åº¦ã€‚"

        report_content = f"""# ğŸ“Š YOLOv8 æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š

**æµ‹è¯•æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}
**æµ‹è¯•æ•°æ®é›†**: `{self.data_yaml}`
**è®¡ç®—è®¾å¤‡**: `{'CUDA (GPU)' if torch.cuda.is_available() else 'CPU'}`

## 1. æ€§èƒ½å¯¹æ¯”è¡¨æ ¼

{md_table}

## 2. è¯¦ç»†æŒ‡æ ‡è¯´æ˜
*   **mAP 50-95**: å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆIoUåœ¨æ­¤èŒƒå›´å†…ï¼‰ï¼Œç»¼åˆåæ˜ æ£€æµ‹å‡†ç¡®ç‡ã€‚
*   **FPS**: æ¯ç§’å¤„ç†å¸§æ•°ï¼Œåæ˜ æ¨ç†é€Ÿåº¦ã€‚å¤§äº 30 é€šå¸¸è§†ä¸ºå®æ—¶ã€‚
*   **Params**: æ¨¡å‹å‚æ•°é‡ï¼Œåæ˜ æ¨¡å‹å¤æ‚åº¦ã€‚

## 3. ğŸ† æœ€ä½³æ¨¡å‹æ¨è

**æ¨èæ¨¡å‹**: **{recommended_model['Model']}**

**æ¨èç†ç”±**: {reason}

*   å¦‚æœä½ è¿½æ±‚**æè‡´ç²¾åº¦**ï¼Œå¯ä»¥é€‰æ‹© **{best_acc_model['Model']}** (mAP: {best_acc_model['mAP 50-95']})ã€‚
*   å¦‚æœä½ è¿½æ±‚**æè‡´é€Ÿåº¦**ï¼Œå¯ä»¥é€‰æ‹© **{fastest_model['Model']}** (FPS: {fastest_model['FPS']})ã€‚
"""
        
        report_path = self.results_dir / 'benchmark_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
            
        logger.info(f"\nğŸ“ Report generated successfully: {report_path}")
        print("\n" + report_content) # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°


def main():
    parser = argparse.ArgumentParser(description="Task 3: YOLOv8 Performance Benchmark")
    parser.add_argument('--data', type=str, default='data/custom_dataset/dataset.yaml',
                        help="æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„ (yaml)")
    args = parser.parse_args()
    
    benchmark = ModelBenchmark(data_yaml=args.data)
    benchmark.run_benchmark()

if __name__ == "__main__":
    main()
